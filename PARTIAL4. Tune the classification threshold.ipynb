{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4. Tune the classification threshold.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_FW_Mx_cVLz"
      },
      "source": [
        "# Load the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "teoeNgNo_AWs",
        "outputId": "66a20ac6-2c70-4668-ff58-35cf4eab2e3d"
      },
      "source": [
        "# Mount Google Drive locally \r\n",
        "# Using the instructions found here https://colab.research.google.com/notebooks/io.ipynb#scrollTo=u22w3BFiOveA&line=1&uniqifier=1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcMvx84U_Cpv"
      },
      "source": [
        "# Load the pickled dataframe using the \"read_pickle()\"\" function from \"pandas\"\r\n",
        "# df = ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWSn5qR8VoNM"
      },
      "source": [
        "# install AIF360\r\n",
        "!pip install aif360"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRxtHR6CfAQC"
      },
      "source": [
        "# Tune the thershold parameter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8aS7IvIaC28"
      },
      "source": [
        "# Instantiate the classifier\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "# lr = ...\r\n",
        "\r\n",
        "# instantiate the cross-validation scheme\r\n",
        "from sklearn.model_selection import StratifiedKFold\r\n",
        "# mv = ...\r\n",
        "\r\n",
        "# setup the performance metrics to be computed\r\n",
        "from sklearn import metrics\r\n",
        "# perf_metrics = {\"Accuracy\": ..., \r\n",
        "#                 \"Precision\": ..., \r\n",
        "#                 \"Recall\": ...,\r\n",
        "#                 \"AUC\": ..., \r\n",
        "#                 \"F1-Score\": ..., \r\n",
        "#                 }\r\n",
        "\r\n",
        "# Set up the fairness metrics to be computed using AIF360\r\n",
        "from aif360.datasets import BinaryLabelDataset\r\n",
        "from aif360.metrics import BinaryLabelDatasetMetric\r\n",
        "from aif360.metrics import ClassificationMetric\r\n",
        "\r\n",
        "# privileged_group = ...\r\n",
        "# unprivileged_group = ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMprXCLmo_J4"
      },
      "source": [
        "# Tune the threshold using 10 fold cross validation\r\n",
        "\r\n",
        "thresholds = [0.3, 0.4, 0.45, 0.5, 0.55, 0.6, 0.7]\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "k, i = True, 0\r\n",
        "\r\n",
        "# instantiating X\r\n",
        "# X = ...\r\n",
        "\r\n",
        "# instantiating the target variable\r\n",
        "# y = ...\r\n",
        "\r\n",
        "for fold_threshold in thresholds:\r\n",
        "  av_f1 = 0\r\n",
        "  av_acc = 0\r\n",
        "  av_auc = 0\r\n",
        "  av_precision = 0\r\n",
        "  av_recall = 0\r\n",
        "  av_statpar = 0\r\n",
        "  av_av_odds = 0\r\n",
        "\r\n",
        "\r\n",
        "  for (train, test) in mv.split(X, y):\r\n",
        "\r\n",
        "      # fit model\r\n",
        "      # lr = lr.fit(...)\r\n",
        "      \r\n",
        "      # get predictions in the test set\r\n",
        "      # ypred_prob = lr.predict_proba ... # get probabilities\r\n",
        "      # ypred_class = lr.predict ...\r\n",
        "\r\n",
        "      # compute performance metrics\r\n",
        "      metrics = []\r\n",
        "      metrics += [['Threshold', fold_threshold]]\r\n",
        "      dataset = X.iloc[test].copy()\r\n",
        "      dataset[df.columns[-1]] = np.expand_dims(y.iloc[test], axis=1)\r\n",
        "      # dataset = BinaryLabelDataset(...)\r\n",
        "      dataset_pred = X.iloc[test].copy()\r\n",
        "      dataset_pred[df.columns[-1]] = np.expand_dims(ypred_class, axis=1)\r\n",
        "      # dataset_pred = BinaryLabelDataset(...)\r\n",
        "      # metric_CM = ClassificationMetric(...)\r\n",
        "\r\n",
        "      av_f1 += perf_metrics[\"F1-Score\"](y.iloc[test].values.ravel(), ypred_class)\r\n",
        "      av_acc += perf_metrics[\"Accuracy\"](y.iloc[test].values.ravel(), ypred_class)\r\n",
        "      av_auc += perf_metrics[\"AUC\"](y.iloc[test].values.ravel(), ypred_class)\r\n",
        "      av_precision += perf_metrics[\"Precision\"](y.iloc[test].values.ravel(), ypred_class)\r\n",
        "      av_recall += perf_metrics[\"Recall\"](y.iloc[test].values.ravel(), ypred_class)\r\n",
        "      av_statpar = metric_CM.statistical_parity_difference()\r\n",
        "      av_av_odds = metric_CM.average_odds_difference()\r\n",
        "\r\n",
        "    \r\n",
        "  # Compute fairness metrics\r\n",
        "  # metrics += [['Threshold',...]]\r\n",
        "  # metrics += [['F1',...]]\r\n",
        "  # metrics += [['Accuracy',...]]\r\n",
        "  # metrics += [['AUC',...]]\r\n",
        "  # metrics += [['Precision',...]]\r\n",
        "  # metrics += [['Recall',...]]\r\n",
        "  # metrics += [['Statistical Parity Difference',...]]\r\n",
        "  # metrics += [['Average Odds Difference',...]]\r\n",
        "\r\n",
        "\r\n",
        "  # concatenate results\r\n",
        "  df_m = pd.DataFrame(metrics, columns=[\"Metric\", \"Value\"])\r\n",
        "  df_m[\"Fold\"] = i\r\n",
        "  i += 1\r\n",
        "  if k:\r\n",
        "      df_metrics = df_m.copy()\r\n",
        "      k=0\r\n",
        "  else:\r\n",
        "      df_metrics = pd.concat([df_metrics, df_m.copy()], axis=0, ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_G02tW8sxrP"
      },
      "source": [
        "# Display metrics\r\n",
        "# df_pivot = ...\r\n",
        "df_pivot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYLig4Pn6TpQ"
      },
      "source": [
        "# Plot metrics against the threshold values\r\n",
        "import seaborn as sns\r\n",
        "y_vars = ['Threshold']\r\n",
        "x_vars = ['Average Odds Difference', 'Statistical Parity Difference', 'AUC', 'Accuracy', 'F1', 'Precision', 'Recall']\r\n",
        "\r\n",
        "g = sns.PairGrid(df_pivot, x_vars=x_vars, y_vars=y_vars)\r\n",
        "g.map(sns.scatterplot)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DijsYvlCyyzT"
      },
      "source": [
        "# Re-run post-processing with two different classification thresholds."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUOKg0NIyh7x"
      },
      "source": [
        "# Instantiate the classifier\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "# lr = ...\r\n",
        "\r\n",
        "# instantiate the cross-validation scheme\r\n",
        "from sklearn.model_selection import StratifiedKFold\r\n",
        "# mv = ...\r\n",
        "\r\n",
        "# setup the performance metrics to be computed\r\n",
        "from sklearn import metrics\r\n",
        "# perf_metrics = {\"Accuracy\": ..., \r\n",
        "#                 \"Precision\": ..., \r\n",
        "#                 \"Recall\": ...,\r\n",
        "#                 \"AUC\": ..., \r\n",
        "#                 \"F1-Score\": ..., \r\n",
        "#                 }\r\n",
        "\r\n",
        "# Set up the fairness metrics to be computed using AIF360\r\n",
        "from aif360.datasets import BinaryLabelDataset\r\n",
        "from aif360.metrics import BinaryLabelDatasetMetric\r\n",
        "from aif360.metrics import ClassificationMetric\r\n",
        "\r\n",
        "# privileged_group = ...\r\n",
        "# unprivileged_group = ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wo8DyusryuXo"
      },
      "source": [
        "from aif360.algorithms.postprocessing import EqOddsPostprocessing\r\n",
        "\r\n",
        "# Learn parameters to equalize odds and apply to create a new dataset\r\n",
        "# epp = EqOddsPostprocessing(...)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reON-6f7yiGY"
      },
      "source": [
        "# Train a logistic regression classifier with two different thresholds\r\n",
        "\r\n",
        "thresholds = [0.45, 0.5]\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "k, i = True, 0\r\n",
        "\r\n",
        "# instantiating X\r\n",
        "# X = ...\r\n",
        "\r\n",
        "# instantiating the target variable\r\n",
        "# y = ...\r\n",
        "\r\n",
        "for fold_threshold in thresholds:\r\n",
        "  av_f1 = 0\r\n",
        "  av_acc = 0\r\n",
        "  av_auc = 0\r\n",
        "  av_precision = 0\r\n",
        "  av_recall = 0\r\n",
        "  av_statpar = 0\r\n",
        "  av_av_odds = 0\r\n",
        "\r\n",
        "  for (train, test) in mv.split(X, y):\r\n",
        "\r\n",
        "      # fit model\r\n",
        "      # lr = lr.fit(...)\r\n",
        "      \r\n",
        "      # get predictions in the test set\r\n",
        "      # ypred_prob = lr.predict_proba ...\r\n",
        "      # ypred_class = ...\r\n",
        "      # y_test = ...\r\n",
        "      # X_test = ...\r\n",
        "\r\n",
        "      # fit post-processing using results from 60% of the test set\r\n",
        "      test_pct = 0.4\r\n",
        "      n = int(len(y_test))\r\n",
        "      n_2 = int(n* (1-test_pct))\r\n",
        "      indices = np.random.permutation(n)\r\n",
        "      pp_indices = indices[:n_2]\r\n",
        "      test_indices = indices[n_2:]\r\n",
        "\r\n",
        "      pp_dataset = X_test.iloc[pp_indices].copy()\r\n",
        "      pp_dataset[df.columns[-1]] = np.expand_dims(y_test.iloc[pp_indices], axis=1)\r\n",
        "      pp_dataset_BLD = BinaryLabelDataset(df=pp_dataset, label_names=['action_taken_name'], protected_attribute_names=['applicant_sex_name_Female'])\r\n",
        "\r\n",
        "      pp_pred = X_test.iloc[pp_indices].copy()\r\n",
        "      pp_pred[df.columns[-1]] = np.expand_dims(ypred_class[pp_indices], axis=1)\r\n",
        "      pp_pred_BLD = BinaryLabelDataset(df=pp_pred, label_names=[df.columns[-1]], protected_attribute_names=['applicant_sex_name_Female'])\r\n",
        "\r\n",
        "      epp = epp.fit(pp_dataset_BLD, pp_pred_BLD)\r\n",
        "\r\n",
        "      # Use epp to post-process predictions on the other 40% of the test set\r\n",
        "      actuals_test = X_test.iloc[test_indices].copy()\r\n",
        "      actuals_test[df.columns[-1]] = np.expand_dims(y_test.iloc[test_indices], axis=1)\r\n",
        "      actuals_test_BLD = BinaryLabelDataset(df=actuals_test, label_names=['action_taken_name'], protected_attribute_names=['applicant_sex_name_Female'])\r\n",
        "      pred_test = X_test.iloc[test_indices].copy()\r\n",
        "      pred_test[df.columns[-1]] = np.expand_dims(ypred_class[test_indices], axis=1)\r\n",
        "      pred_test_BLD = BinaryLabelDataset(df=pred_test, label_names=[df.columns[-1]], protected_attribute_names=['applicant_sex_name_Female'])\r\n",
        "\r\n",
        "      transf_pred_test_BLD = epp.predict(pred_test_BLD)\r\n",
        "\r\n",
        "      # compute performance metrics\r\n",
        "      metrics = []\r\n",
        "      metrics += [['Threshold', fold_threshold]]\r\n",
        "      # metric_CM = ClassificationMetric(...)\r\n",
        "\r\n",
        "      av_f1 += perf_metrics[\"F1-Score\"](y.iloc[test].values.ravel(), ypred_class)\r\n",
        "      av_acc += perf_metrics[\"Accuracy\"](y.iloc[test].values.ravel(), ypred_class)\r\n",
        "      av_auc += perf_metrics[\"AUC\"](y.iloc[test].values.ravel(), ypred_class)\r\n",
        "      av_precision += perf_metrics[\"Precision\"](y.iloc[test].values.ravel(), ypred_class)\r\n",
        "      av_recall += perf_metrics[\"Recall\"](y.iloc[test].values.ravel(), ypred_class)\r\n",
        "      av_statpar = metric_CM.statistical_parity_difference()\r\n",
        "      av_av_odds = metric_CM.average_odds_difference()\r\n",
        "\r\n",
        "    \r\n",
        "  # Compute fairness metrics\r\n",
        "  # metrics += [['Threshold', ...]]\r\n",
        "  # metrics += [['F1', ...]]\r\n",
        "  # metrics += [['Accuracy', ...]]\r\n",
        "  # metrics += [['AUC', ...]]\r\n",
        "  # metrics += [['Precision', ...]]\r\n",
        "  # metrics += [['Recall', ...]]\r\n",
        "  # metrics += [['Statistical Parity Difference', ...]]\r\n",
        "  # metrics += [['Average Odds Difference', ...]]\r\n",
        "\r\n",
        "\r\n",
        "  # concatenate results\r\n",
        "  df_m = pd.DataFrame(metrics, columns=[\"Metric\", \"Value\"])\r\n",
        "  df_m[\"Fold\"] = i\r\n",
        "  i += 1\r\n",
        "  if k:\r\n",
        "      df_metrics = df_m.copy()\r\n",
        "      k=0\r\n",
        "  else:\r\n",
        "      df_metrics = pd.concat([df_metrics, df_m.copy()], axis=0, ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idS10jPb0JiM"
      },
      "source": [
        "# Display metrics\r\n",
        "#  df_pivot = ...\r\n",
        "df_pivot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBARBoqEr8Ow"
      },
      "source": [
        "The model that minimises false-negatives is the model with a threshold of 0.45."
      ]
    }
  ]
}