{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2. Compute and chart bias metrics on a basic classifier.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_Nr1tfBBTxX"
      },
      "source": [
        "# install AIF360\r\n",
        "!pip install aif360"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSPgXiRF0HZw"
      },
      "source": [
        "# Instantiate the classifier\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "# lr = ...\r\n",
        "\r\n",
        "# instantiate the cross-validation scheme\r\n",
        "from sklearn.model_selection import StratifiedKFold\r\n",
        "# mv = ...\r\n",
        "\r\n",
        "# setup the metrics to be computed\r\n",
        "from sklearn import metrics\r\n",
        "# perf_metrics = {\"Accuracy\": ..., \r\n",
        "#                 \"Precision\": ..., \r\n",
        "#                 \"Recall\": ...,\r\n",
        "#                 \"AUC\": ..., \r\n",
        "#                 \"F1-Score\": ..., \r\n",
        "#                 }\r\n",
        "\r\n",
        "# Set up the fairness metrics to be computed using AIF360\r\n",
        "from aif360.datasets import BinaryLabelDataset\r\n",
        "from aif360.metrics import BinaryLabelDatasetMetric\r\n",
        "from aif360.metrics import ClassificationMetric\r\n",
        "\r\n",
        "# privileged_group = ...\r\n",
        "# unprivileged_group = ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_-qoMNG0My7"
      },
      "source": [
        "# Train a logistic regression classifier on the dataset and calculate metrics\r\n",
        "\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "k, i = True, 1\r\n",
        "\r\n",
        "# instantiating X\r\n",
        "X = df.drop(labels=df.columns[-1], axis=1)\r\n",
        "\r\n",
        "# instantiating the target variable\r\n",
        "y = df[df.columns[-1]]\r\n",
        "\r\n",
        "for (train, test) in mv.split(X, y):\r\n",
        "    # fit model\r\n",
        "    # lr = ...\r\n",
        "    \r\n",
        "    # get predictions in the test set\r\n",
        "    # ypred_prob = lr.predict_proba ... # get probabilities\r\n",
        "    # ypred_class = lr.predict ...\r\n",
        "\r\n",
        "    # compute performance metrics\r\n",
        "    metrics = []\r\n",
        "    dataset = X.iloc[test].copy()\r\n",
        "    dataset[df.columns[-1]] = np.expand_dims(y.iloc[test], axis=1)\r\n",
        "    # dataset = BinaryLabelDataset(...)\r\n",
        "    dataset_pred = X.iloc[test].copy()\r\n",
        "    dataset_pred[df.columns[-1]] = np.expand_dims(ypred_class, axis=1)\r\n",
        "    # dataset_pred = BinaryLabelDataset(...)\r\n",
        "    # metric_BLDM = BinaryLabelDatasetMetric(...)\r\n",
        "    # metric_CM = ClassificationMetric(...)\r\n",
        "    for pf in perf_metrics.keys():\r\n",
        "        if pf in [\"AUC\", \"Brier\"]:\r\n",
        "            # metrics += [[pf, ...]]\r\n",
        "        else:\r\n",
        "            # metrics += [[pf, ...]]\r\n",
        "    \r\n",
        "    # Compute fairness metrics\r\n",
        "    #metrics += [['Statistical Parity Difference', ...]]\r\n",
        "    #metrics += [['Disparate Impact', ...]]\r\n",
        "    #metrics += [['Equal Opportunity Difference', ...]]\r\n",
        "    #metrics += [['Average Odds Difference', ...]]\r\n",
        "    #metrics += [['Accuracy Male', ...]]\r\n",
        "    #metrics += [['Accuracy Female', ...]]\r\n",
        "\r\n",
        "\r\n",
        "    # concatenate results\r\n",
        "    df_m = pd.DataFrame(metrics, columns=[\"Metric\", \"Value\"])\r\n",
        "    df_m[\"Fold\"] = i\r\n",
        "    i += 1\r\n",
        "    if k:\r\n",
        "        df_metrics = df_m.copy()\r\n",
        "        k=0\r\n",
        "    else:\r\n",
        "        df_metrics = pd.concat([df_metrics, df_m.copy()], axis=0, ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWpZ1COu5f8c"
      },
      "source": [
        "# Display metrics\r\n",
        "# metrics_table = df_metrics.pivot_table(...)\r\n",
        "metrics_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2htyxfjFcHW"
      },
      "source": [
        "# Construct a DataFrame with allowable thresholds for plotting\r\n",
        "\r\n",
        "fairness_df = pd.DataFrame(columns=[\"Metric\", \"Value\", \"Min\", \"Max\"])\r\n",
        "# fairness_df.loc[0] = ['Disparate Impact'] + ...\r\n",
        "# fairness_df.loc[1] = ['Statistical Parity Difference'] + ...\r\n",
        "# fairness_df.loc[2] = ['Average Odds Difference'] + ...\r\n",
        "# fairness_df.loc[3] = ['Equal Opportunity Difference'] + ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcATkC0eGipx"
      },
      "source": [
        "# Chart the metrics\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import seaborn as sns\r\n",
        "import matplotlib.patches as patches\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "def plot_metrics(df_fair): \r\n",
        "  def plot_metric(metric, ylim):\r\n",
        "    fig, ax = plt.subplots()\r\n",
        "    ax = sns.barplot(data=metric, x=\"Metric\", y=\"Value\")\r\n",
        "    plt.axhline(np.mean(metric[[\"Min\", \"Max\"]].values), color='black')\r\n",
        "    plt.ylim(*ylim)\r\n",
        "    range_height = metric[[\"Max\"]].values[0][0] - metric[[\"Min\"]].values[0][0]\r\n",
        "    ax.add_patch(patches.Rectangle((-1, metric[[\"Min\"]].values[0][0]), 2 , range_height, facecolor=\"green\", alpha=0.3))\r\n",
        "\r\n",
        "  plot_metric(df_fair.iloc[[0]], (0, 2))\r\n",
        "  plot_metric(df_fair.iloc[[1]], (-1, 1))\r\n",
        "  plot_metric(df_fair.iloc[[2]], (-1, 1))\r\n",
        "  plot_metric(df_fair.iloc[[3]], (-1, 1))\r\n",
        "\r\n",
        "plot_metrics(fairness_df)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}