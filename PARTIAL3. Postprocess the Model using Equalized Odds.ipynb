{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3. Postprocess the Model using Equalized Odds.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKjIHRwzZ6pC"
      },
      "source": [
        "# Load the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9LLgxOGx7-W"
      },
      "source": [
        "# Mount Google Drive locally \r\n",
        "# Using the instructions found here https://colab.research.google.com/notebooks/io.ipynb#scrollTo=u22w3BFiOveA&line=1&uniqifier=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRrtE1Vxx-Cz"
      },
      "source": [
        "# Load the pickled dataframe using the \"read_pickle()\"\" function from \"pandas\"\r\n",
        "# df = ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rD2QZf-3Z9M0"
      },
      "source": [
        "# Install AIF360"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DX8wyVUNx-Q3"
      },
      "source": [
        "# install AIF360\r\n",
        "!pip install aif360"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DL23rLV-Z__h"
      },
      "source": [
        "# Train a classifier, implement post-processing, and compute metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiyz85N3Dx7j"
      },
      "source": [
        "# Instantiate the classifier\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "# lr = ...\r\n",
        "\r\n",
        "# instantiate the cross-validation scheme\r\n",
        "from sklearn.model_selection import StratifiedKFold\r\n",
        "# mv = ...\r\n",
        "\r\n",
        "# setup the performance metrics to be computed\r\n",
        "from sklearn import metrics\r\n",
        "# perf_metrics = {\"Accuracy\": ..., \r\n",
        "#                 \"Precision\": ..., \r\n",
        "#                 \"Recall\": ...,\r\n",
        "#                 \"AUC\": ..., \r\n",
        "#                 \"F1-Score\": ..., \r\n",
        "#                 }\r\n",
        "\r\n",
        "# Set up the fairness metrics to be computed using AIF360\r\n",
        "from aif360.datasets import BinaryLabelDataset\r\n",
        "from aif360.metrics import BinaryLabelDatasetMetric\r\n",
        "from aif360.metrics import ClassificationMetric\r\n",
        "\r\n",
        "# privileged_group = ...\r\n",
        "# unprivileged_group = ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POGm9gUgJ-Ka"
      },
      "source": [
        "from aif360.algorithms.postprocessing import EqOddsPostprocessing\r\n",
        "\r\n",
        "# Learn parameters to equalize odds and apply to create a new dataset\r\n",
        "# epp = EqOddsPostprocessing(...)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhOMtPB-J8uI"
      },
      "source": [
        "# Train a logistic regression classifier on the dataset and calculate metrics\r\n",
        "\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "k, i = True, 1\r\n",
        "\r\n",
        "# instantiating X\r\n",
        "# X = ...\r\n",
        "\r\n",
        "# instantiating the target variable\r\n",
        "# y = ...\r\n",
        "\r\n",
        "for (train, test) in mv.split(X, y):\r\n",
        "    # X_train = ...\r\n",
        "    # y_train = ...\r\n",
        "    # fit model\r\n",
        "    # lr = lr.fit(...)\r\n",
        "   \r\n",
        "    # get predictions in the test set\r\n",
        "    # X_test = ...\r\n",
        "    # y_test = ...\r\n",
        "    # ypred_prob = lr.predict_proba ... # get probabilities\r\n",
        "    # ypred_class = lr.predict ...\r\n",
        "\r\n",
        "    # fit post-processing using results from 60% of the test set\r\n",
        "    test_pct = 0.4\r\n",
        "    n = int(len(y_test))\r\n",
        "    n_2 = int(n* (1-test_pct))\r\n",
        "    indices = np.random.permutation(n)\r\n",
        "    pp_indices = indices[:n_2]\r\n",
        "    test_indices = indices[n_2:]\r\n",
        "\r\n",
        "    pp_dataset = X_test.iloc[pp_indices].copy()\r\n",
        "    pp_dataset[df.columns[-1]] = np.expand_dims(y_test.iloc[pp_indices], axis=1)\r\n",
        "    # pp_dataset_BLD = BinaryLabelDataset(...)\r\n",
        "\r\n",
        "    pp_pred = X_test.iloc[pp_indices].copy()\r\n",
        "    pp_pred[df.columns[-1]] = np.expand_dims(ypred_class[pp_indices], axis=1)\r\n",
        "    # pp_pred_BLD = BinaryLabelDataset(...)\r\n",
        "\r\n",
        "    # epp = epp.fit(...)\r\n",
        "\r\n",
        "    # Use epp to post-process predictions on the other 40% of the test set\r\n",
        "    actuals_test = X_test.iloc[test_indices].copy()\r\n",
        "    actuals_test[df.columns[-1]] = np.expand_dims(y_test.iloc[test_indices], axis=1)\r\n",
        "    # actuals_test_BLD = BinaryLabelDataset(...)\r\n",
        "    pred_test = X_test.iloc[test_indices].copy()\r\n",
        "    pred_test[df.columns[-1]] = np.expand_dims(ypred_class[test_indices], axis=1)\r\n",
        "    # pred_test_BLD = BinaryLabelDataset(...)\r\n",
        "\r\n",
        "    # transf_pred_test_BLD = epp.predict(...)\r\n",
        "\r\n",
        "    # compute performance metrics\r\n",
        "    metrics = []\r\n",
        "    # metric_CM = ClassificationMetric(...)\r\n",
        "    for pf in perf_metrics.keys():\r\n",
        "        if pf in [\"AUC\", \"Brier\"]:\r\n",
        "            # metrics += [[pf, ...]]\r\n",
        "        else:\r\n",
        "            # metrics += [[pf, ...]]\r\n",
        "    \r\n",
        "    # Compute fairness metrics\r\n",
        "    # metrics += [['Statistical Parity Difference', ...]]\r\n",
        "    # metrics += [['Disparate Impact', ...]]\r\n",
        "    # metrics += [['Equal Opportunity Difference', ...]]\r\n",
        "    # metrics += [['Average Odds Difference', ...]]\r\n",
        "    # metrics += [['Accuracy Male', ...]]\r\n",
        "    # metrics += [['Accuracy Female', ...]]\r\n",
        "\r\n",
        "\r\n",
        "    # concatenate results\r\n",
        "    df_m = pd.DataFrame(metrics, columns=[\"Metric\", \"Value\"])\r\n",
        "    df_m[\"Fold\"] = i\r\n",
        "    i += 1\r\n",
        "    if k:\r\n",
        "        df_metrics = df_m.copy()\r\n",
        "        k=0\r\n",
        "    else:\r\n",
        "        df_metrics = pd.concat([df_metrics, df_m.copy()], axis=0, ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rx3iebX-aSjR"
      },
      "source": [
        "# Display the metrics in a table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TA3CnMKRKkwJ"
      },
      "source": [
        "# Display metrics\r\n",
        "# metrics_table = df_metrics.pivot_table(...)\r\n",
        "metrics_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bv_oKO_tagCR"
      },
      "source": [
        "# Chart the metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbkdr5VmCk6w"
      },
      "source": [
        "# Construct a DataFrame with allowable thresholds for plotting\r\n",
        "\r\n",
        "fairness_df = pd.DataFrame(columns=[\"Metric\", \"Value\", \"Min\", \"Max\"])\r\n",
        "fairness_df.loc[0] = ['Disparate Impact'] + list((metrics_table[('mean', 'Value')]['Disparate Impact'], 0.8, 1.2))\r\n",
        "fairness_df.loc[1] = ['Statistical Parity Difference'] + list((metrics_table[('mean', 'Value')]['Statistical Parity Difference'], -0.1, 0.1))\r\n",
        "fairness_df.loc[2] = ['Average Odds Difference'] + list((metrics_table[('mean', 'Value')]['Average Odds Difference'], -0.1, 0.1))\r\n",
        "fairness_df.loc[3] = ['Equal Opportunity Difference'] + list((metrics_table[('mean', 'Value')]['Equal Opportunity Difference'], -0.1, 0.1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTymsKH3C5Yb"
      },
      "source": [
        "# Chart the metrics\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import seaborn as sns\r\n",
        "import matplotlib.patches as patches\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "def plot_metrics(df_fair): \r\n",
        "  def plot_metric(metric, ylim):\r\n",
        "    fig, ax = plt.subplots()\r\n",
        "    ax = sns.barplot(data=metric, x=\"Metric\", y=\"Value\")\r\n",
        "    plt.axhline(np.mean(metric[[\"Min\", \"Max\"]].values), color='black')\r\n",
        "    plt.ylim(*ylim)\r\n",
        "    range_height = metric[[\"Max\"]].values[0][0] - metric[[\"Min\"]].values[0][0]\r\n",
        "    ax.add_patch(patches.Rectangle((-1, metric[[\"Min\"]].values[0][0]), 2 , range_height, facecolor=\"green\", alpha=0.3))\r\n",
        "\r\n",
        "  plot_metric(df_fair.iloc[[0]], (0, 2))\r\n",
        "  plot_metric(df_fair.iloc[[1]], (-1, 1))\r\n",
        "  plot_metric(df_fair.iloc[[2]], (-1, 1))\r\n",
        "  plot_metric(df_fair.iloc[[3]], (-1, 1))\r\n",
        "\r\n",
        "plot_metrics(fairness_df)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}