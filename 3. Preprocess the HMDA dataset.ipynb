{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3. Preprocess the HMDA dataset.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM+WuV5894yFCWhkdWJEraG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/umarmohammed/measuring-bias-in-a-dataset/blob/project-1/3.%20Preprocess%20the%20HMDA%20dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ElQO6HmIwed2",
        "outputId": "2667a124-0053-4eb4-df21-ab16e8ec90b2"
      },
      "source": [
        "# Mount Google Drive locally\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5VMvi5Wynl3"
      },
      "source": [
        "# Load the dataset into a pandas dataframe\n",
        "import pandas as pd\n",
        "df = pd.read_pickle(\"/content/drive/My Drive/liveProject/mortgage_data_balanced.pkl.gz\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6GCfIjtyqEG"
      },
      "source": [
        "# Remove extra columns\n",
        "keep_vars = ['respondent_id', 'as_of_year', 'agency_abbr', 'loan_type_name', 'loan_amount_000s', 'owner_occupancy_name',\n",
        "             'loan_purpose_name', 'property_type_name', 'preapproval_name', 'msamd_name', 'state_abbr', 'county_name',\n",
        "             'applicant_ethnicity_name', 'co_applicant_ethnicity_name', 'applicant_race_name_1', 'co_applicant_race_name_1',\n",
        "             'applicant_sex_name', 'co_applicant_sex_name', 'applicant_income_000s', 'purchaser_type_name', \n",
        "             'denial_reason_name_1', 'hoepa_status_name', 'lien_status_name', 'population', 'minority_population',\n",
        "             'hud_median_family_income', 'tract_to_msamd_income', 'number_of_owner_occupied_units', \n",
        "             'number_of_1_to_4_family_units', 'action_taken_name']\n",
        "df = df[keep_vars].copy()\n",
        "\n",
        "\n",
        "# categorical variables\n",
        "cat_variables = cat_variables = ['applicant_ethnicity_name', 'applicant_race_name_1', 'applicant_sex_name', 'agency_abbr',\n",
        "                                 'owner_occupancy_name', 'property_type_name', 'loan_purpose_name', 'loan_type_name']\n",
        "\n",
        "# other integer variables\n",
        "int_variables = ['loan_amount_000s', 'applicant_income_000s', 'population', 'minority_population', \n",
        "                 'hud_median_family_income', 'tract_to_msamd_income', 'number_of_owner_occupied_units', \n",
        "                 'number_of_1_to_4_family_units']\n",
        "\n",
        "# target variable\n",
        "output_variable = ['action_taken_name']\n",
        "\n",
        "### Pre-processing\n",
        "# Mapping categorical variables to one-hot encoding\n",
        "df_cat = pd.DataFrame(index=df.index)\n",
        "\n",
        "# one-hot encoding of categorical variables\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# I will do a loop for pedagogical reasons, but it is not entirely necessary\n",
        "for cat in cat_variables:\n",
        "    # one-hot encoding fitting\n",
        "    one_hot_func = OneHotEncoder().fit(df[[cat]])\n",
        "    \n",
        "    # mapping\n",
        "    cat_mapped = one_hot_func.transform(df[[cat]]).toarray()\n",
        "    \n",
        "    # storing\n",
        "    for (k, cat_label) in enumerate(one_hot_func.categories_[0]):\n",
        "        df_cat[cat + \"_\" + cat_label] = cat_mapped[:, k]\n",
        "\n",
        "# consolidating a final dataset\n",
        "X = pd.concat([df[int_variables], df_cat], axis=1)\n",
        "y = (df[output_variable] == \"Application denied by financial institution\").copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwEEHR5kyvD8"
      },
      "source": [
        "# Store the dataset in pickled dictionary\n",
        "import pickle\n",
        "\n",
        "data = {\"X\": X, \"y\": y}\n",
        "\n",
        "with open(\"/content/drive/My Drive/liveProject/mortgage_data_preprocess.pkl.gz\", 'wb') as pickle_file:\n",
        "    pickle.dump(data, pickle_file)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}